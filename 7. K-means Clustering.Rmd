---
title: "K-means Clustering"
output:
  word_document: default
  rmarkdown::pdf_document:
    fig_caption: yes
    includes:
      in_header: preamble-latex.tex
  pdf_document:
    fig_caption: yes
    fig_crop: no
    keep_tex: yes
    number_sections: yes
header-includes:
- \usepackage{graphicx}
- \usepackage{float}
urlcolor: blue
---
```{r echo=FALSE,warning=FALSE}
library(knitr)
opts_chunk$set(cache.path='cache/graphics-',
               fig.align='center',
               external=TRUE,
               echo=TRUE,
               warning=FALSE,
               fig.pos='H'
               )
```

# Short Intro to k-means

K-means clustering is an unsupervised machine learning algorithm. Unsupervised algorithms make inferences using only input data without referring to known, or labeled outcomes. The goal of the K-means is to group similar observations together and discover underlying patterns.

K-means algorithm has one input parameter - **k** - which refers to the number of groups (clusters) that the observations should be grouped into. The algorithm actually finds centers of those clusters, which are called *centroids*; it allocates each observation to a cluster based the nearest cluster centroid. The objective of the K-means is to minimize the in-cluster sum of squares, that is the sum of the squared distance of every point to its corresponding cluster centroid.

Steps of the K-means algorithm:

1) The initial selection of cluster centroids. Centroids are either randomly generated or selected from the data set, i.e. K random observations are declared as centroids

Repeat:

2) Cluster assignment: For each observation from the data set, identify the nearest centroid (usually based on the Euclidean distance) and assign the observation to the cluster of that centroid.

3) Centroid update: for each cluster, calculate a new centroid by taking the mean of all observations assigned to that cluster.
  
The algorithm iterates between steps 2 and 3 until one of the following conditions is met: i) no observation changes its cluster; ii) the sum of the distances is minimized (that is, falls beneath the threshold); iii) the maximum number of iterations is reached.

The result of the algorithm may be a local optimum that is not necessarily the best possible outcome. Therefore k-means algorithm is typically run several times, each time with a different (randomized) set of initial centroids. Eventually, the result of the best run is consider the final one.

## K-means Example

Suppose we have a dataset with several observations and two variables. They can be plotted in the Euclidean space (Fig. 1). 

```{r fig.align='center', echo=FALSE, out.width = "350px", fig.cap="Initial data and Initial selection of centroids"}
knitr::include_graphics(rep('images/steps_0_1.png'))
```
In the **Step 1** of the algorithm, we randomly choose the centroids. Suppose k=2, which means we need to choose two centroids. Although there are other approaches to choosing centroids, we will use the most basic one: choosing randomly. Still, it is advisable to generate the most disperse values possible (maximizing distance between centroids) in order to prevent unwanted localized convergence.

In the **Step 2**, we assign a cluster to each observation (Fig. 2). The assignment is performed based on the Euclidean distance measured from a data point (representing the observation) to centroids. An observation is assigned the cluster of the nearest centroid.

In the **Step 3**, we update the cluster centroids and assign them a new value based on the mean value of all observations that belong to the cluster (Fig. 2).

```{r fig.align='center', echo=FALSE, out.width = "350px", fig.cap="Cluster assignment and centroid update (Iteration 1)"}
knitr::include_graphics(rep('images/steps_2_3_iter1.png'))
```

In the next iteration (Iteration 2), we repeat Steps 2 and 3 (Fig. 3). In **Step 2**, we again assign each data point (observation) to a cluster. We can observe that two data points changed their cluster, i.e. they switched the cluster. After that, in **Step 3**, we again calculate new centroids as the mean value of all the observations that currently belong to the corresponding clusters.

```{r fig.align='center', echo=FALSE, out.width = "350px", fig.cap="Cluster assignment and centroid update (Iteration 2)"}
knitr::include_graphics(rep('images/steps_2_3_iter2.png'))
```

In Iteration 3, none of the observations would change their cluster. This means that the algorithm has converged and that no further iterations should be performed.  

# Load and prepare data set

*Wholesale Customer dataset* contains data about clients of a wholesale distributor. It includes the annual spending in monetary units (m.u.) on diverse product categories. The dataset is available from the [UCI ML Repository](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers). Note that the dataset used in this script is not the original one; the original data were partially preprocessed, which included the transformation of the *Channel* and *Region* attributes into factors as well as removal of outliers for some of the variables.

The objective is to segment (cluster) clients of the wholesale distributor.

Let's load the dataset.

```{r}
# load the data from "data/wholesale_customers.csv"
customers.data <- read.csv(file = "data/wholesale_customers.csv")

# examine the data structure
str(customers.data)
```

## Examining and Preparing the Data

We will first check if there are observations with missing values. If missing values are present, those instances should be either removed or imputed (imputation is the process of replacing missing data with substituted values).

```{r}
# check for missing values
which(complete.cases(customers.data)==FALSE)
```

In our dataset, there are no missing values.

Since the algorithm is based on measuring distances (e.g. Euclidean), this implies that **all variables must be continuous** and the approach can be severely **affected by outliers**. So, we will use only numerical variables and will check if outliers are present. 

Box-plots are useful for the detection of outliers. More details on how to analyze box plots can be found [here](https://flowingdata.com/2008/02/15/how-to-read-and-use-a-box-and-whisker-plot/).

```{r echo=FALSE}
# laod ggplot2 library
library(ggplot2)
```

```{r fig.height=3}
# plot boxplots for all numeric variables
ggplot(customers.data, aes(x=Channel, y=Grocery, fill=Channel)) + geom_boxplot()
ggplot(customers.data, aes(x=Channel, y=Milk, fill=Channel)) + geom_boxplot()
ggplot(customers.data, aes(x=Channel, y=Delicatessen, fill=Channel)) + geom_boxplot()
ggplot(customers.data, aes(x=Channel, y=Frozen, fill=Channel)) + geom_boxplot()
ggplot(customers.data, aes(x=Channel, y=Fresh, fill=Channel)) + geom_boxplot()
ggplot(customers.data, aes(x=Channel, y=Detergents_Paper, fill=Channel)) + geom_boxplot()
```

A couple of variables have outliers namely Grocery, Milk, and Frozen.

The plots also suggest that there is a considerable difference between the two distribution channels, so it would be better to examine and cluster each of them separately.

```{r}
# split the dataset into two subsets based on the value of the Channel variable
retail.data <- subset(customers.data, Channel == 'Retail')
horeca.data <- subset(customers.data, Channel == 'Horeca')
```

In the following, we will focus on the *retail.data*. **For the homework**: do the same with the *horeca.data*.

```{r}
# print the summary of the retail.data
summary(retail.data)
```

Remove the *Channel* variable as we now have just one channel.

```{r}
# remove the Channel variable
retail.data$Channel <- NULL
```

Check which variables have outliers.

```{r}
# compute the number of outliers for all numeric variables
apply(X = retail.data[,-1], # all variables except Region
      MARGIN = 2,
      FUN = function(x) length(boxplot.stats(x)$out))
```

So, *Grocery* and *Frozen* variables have outliers that we need to deal with.

As a way of dealing with outliers, we'll use the [Winsorizing technique](https://en.wikipedia.org/wiki/Winsorizing). Practically, it consists of replacing extreme values with a specific percentile of the data, typically 90th or 95th for overly high values and 5th percentile for overly low values.

Let's start with the *Grocery* variable. We will identify the outliers and sort them by their values.

```{r}
# sort all outliers of the Grocery variable
sort(boxplot.stats(retail.data$Grocery)$out)
```

Now, we examine the 90th, 95th, ... percentile.

```{r}
# examine percentiles of the Grocery variable higher than the 90th percentile
quantile(retail.data$Grocery, probs = seq(from = 0.9, to = 1, by = 0.025))
```

The 95th percentile seems to be a good cutting point since it is not an outlier, but is closest to the smallest outlier (`r sort(boxplot.stats(retail.data$Grocery)$out)[1]`)  

```{r}
# store the 95th percentile of the Grocery variable to a new variable
new.max <- as.numeric(quantile(retail.data$Grocery, probs = 0.95))

# to all outliers of the Grocery variable assing the value of the 95th percentile
retail.data$Grocery[retail.data$Grocery > new.max] <- new.max
```

By drawing the box plot for the *Grocery* variable again we will see that no outliers are present anymore.

```{r fig.height=4, fig.width=4}
# print the boxplot for the Grocery variable
boxplot(retail.data$Grocery, xlab='Grocery')
```

Now, we'll deal, in the same manner, with outliers for the *Frozen* variable.

```{r}
# sort all outliers of the Frozen variable
sort(boxplot.stats(retail.data$Frozen)$out)
```

```{r}
# examine percentiles of the Frozen variable higher than the 90th percentile
quantile(retail.data$Frozen, probs = c(seq(0.9, 1, 0.025)))
```

Setting values to the 92.5th percentile seem to be a good approach.

```{r fig.height=4, fig.width=4}
# store the 92.5th percentile of the Frozen variable to a new variable
new.max <- as.numeric(quantile(retail.data$Frozen, probs = 0.925))

# to all outliers of the Frozen variable assing the value of the 92.5th percentile
retail.data$Frozen[retail.data$Frozen > new.max] <- new.max

# print a boxplot for the Frozen variable
boxplot(retail.data$Frozen, xlab='Frozen')
```

Finally, examine the *retail.data* after the transformations.

```{r}
# print the sumary of the retail.data dataset
summary(retail.data)
```

# Clustering with 2 Features

Let's choose only two features for this initial, simple clustering.

*NOTE:* Clustering with two features only is used here just for the sake of clearer exlanation of the clustering steps and the results; in real-world situations, multiple (numerical) features are used for clustering. 

```{r}
# print the matrix of scatterplots for all numeric variables
pairs(~Fresh+Frozen+Grocery+Milk+Delicatessen+Detergents_Paper, data = retail.data)
```

By looking at the plots, no particular pattern can be observed. But for the sake of an example, let's pick *Frozen* and *Milk* variables. Get a closer look at selected variables.

```{r fig.height=4}
# plot the scatterplot for the variables Frozen and Milk
ggplot(data=retail.data, aes(x=Frozen, y=Milk)) + 
  geom_point() +
  theme_bw()
```

No clear pattern in the data, but let's run K-means and see if some clusters will emerge.

Create a subset of the original data containing the attributes to be used in the K-means.

```{r}
# create a subset of the data with variables Frozen and Milk
retail.data1 <- retail.data[, c("Frozen", "Milk")]

# print the summary of the new dataset
summary(retail.data1)
```

When variables are in incomparable units and/or the numeric values are on very different scales of magnitude, they should be rescaled. Since our variables Frozen and Milk have different value ranges, we need to rescale the data. To that end, we will use normalization as there are no outliers. Normalization is done using the formula:

$$Z = \frac{X - min(X)}{max(X) - min(X)}$$

Let's create a function for performing the normalization.

```{r}
# function for performing the normalization
normalize.feature <- function( feature ) {
  if ( sum(feature, na.rm = T) == 0 ) feature
  else ((feature - min(feature, na.rm = T))/(max(feature, na.rm = T) - min(feature, na.rm = T)))
}
```

Then, we normalize all variables by calling our custom function *normalize.feature.*

```{r}
# normalize both variables
retail.data1.norm <- as.data.frame(apply(retail.data1, 2, normalize.feature))

# print the summary
summary(retail.data1.norm)
```

Run the K Means algorithm using the *kmeans* function

```{r}
# open the documentation for the kmeans function
?kmeans
```

Initially, we will try to group data into, for example, 4 clusters (k=4). We use the *iter.max* parameter to define the maximum number of iterations. This overcomes the problem of slow (or no) convergence. K-means tends to be sensitive to the initial selection of centroids. Thus, *nstart* parameter is used to indicate the number of initial configurations to try (and eventually select the best one). 

Since the initial cluster centers (centroids) are randomly selected, we need to set the seed to assure replicability of the results.
```{r}
# set the seed
set.seed(5320)

# run the clustering with 4 clusters, iter.max=20, nstart=1000
simple.4k <- kmeans(x = retail.data1.norm, centers=4, iter.max=20, nstart=1000)

# print the model
simple.4k
```

From the output, we can observe the following evaluation metrics:

- **within_SS (within-cluster sum of squares)**: The sum of squared differences between individual data points in a cluster and the cluster center (centroid). It is computed for each cluster separately;

- **total_SS (total sum of squares)**: The sum of squared differences of each data point to the global sample mean;

- **between_SS (between cluster sum of squares)**: The sum of squared differences of each centroid to the global sample mean; when computing this value, the squared difference of each cluster center to the global sample mean is multiplied by the number of data points in that cluster;

- **between_SS / total_SS**: This ratio indicates how well the sample splits into clusters; the higher the ratio, the better the clustering. The maximum is 1.

Add the vector of cluster assignments to the data frame as a new variable *cluster* and plot it.

```{r}
# add the cluster as a new variable to the dataset
retail.data1.norm$cluster <- factor(simple.4k$cluster)

# print several instances of the dataset
head(retail.data1.norm)
```

Let's plot the data and new centroids to visually inspect the clusters. 

```{r fig.height=4}
# plot the clusters along with their centroids and color the points by their respective clusters
ggplot(data=retail.data1.norm, aes(x=Frozen, y=Milk, colour=cluster)) + 
  geom_point() + 
  labs(x = "Annual spending on frozen products",
       y = "Annual spending on dairy products",
       title = "Retail customers annual spending") + 
  theme_bw() +
  # add cluster centers
  geom_point(data=as.data.frame(simple.4k$centers), colour="black", size=4, shape=17) 
```

We have set K to 4 by making a random guess. However, we should adopt a more systematic approach to selecting the value for K.

# Selecting the Best Value for K (the Elbow Method)

Instead of guessing the best value for K, we can take a more systematic approach to choosing that value. It is called the **Elbow method**, and is based on the sum of squared differences between data points and cluster centers, that is, the sum of *within_SS* for all the clusters (*tot.withinss*).

Let us run the K-means for different values for K (from 2 to 8). We will compute the metric required for the Elbow method (*tot.withinss*). Along the way, we'll also compute another useful metric: *ratio of between_SS and total_SS*.

```{r}
# create an empty data frame for storing evaluation measures for different k values
eval.metrics.2var <- data.frame()

# remove the column with cluster assignments
retail.data1.norm$cluster <- NULL

# run kmeans for all K values in the range 2:8
for (k in 2:8) {
  set.seed(5320)
  km.res <- kmeans(x=retail.data1.norm, centers=k, iter.max=20, nstart = 1000)
  # combine cluster number and the evaluation measures, write to df
  eval.metrics.2var <- rbind(eval.metrics.2var, 
                             c(k, km.res$tot.withinss, km.res$betweenss/km.res$totss)) 
}

# assign more meaningful column names
names(eval.metrics.2var) <- c("k", "tot.within.ss", "ratio")

# print the metrics
eval.metrics.2var
```

Draw the Elbow plot.

```{r fig.height=4}
# plot the line chart for K values vs. tot.within.ss 
ggplot(data=eval.metrics.2var, aes(x=k, y=tot.within.ss)) + 
  geom_line() +
  labs(x = "\nK (cluster number)", 
       y = "Total Within Cluster Sum of Squares\n",
       title = "Reduction in error for different values of K\n") + 
  theme_bw() +
  scale_x_continuous(breaks=seq(from=0, to=8, by=1))
```

It seems that *K=3* or *K=4* would be the best options for the number of clusters.

If it is not fully clear from the plot where we have a significant decrease in the *tot.within.ss* value, we can compute the difference between every two subsequent values. We will load the script with our utility functions (code is given at the end of the file) and call our custom function *compute.difference*.

```{r}
# load the source code from the Utility.R file
source("Utility.R")

# calculate the difference in tot.within.ss and in ratio for each two consecutive K values (from 2 to 8) 
data.frame(K=2:8, 
           tot.within.ss.delta=compute.difference(eval.metrics.2var$tot.within.ss),
           ratio.delta=compute.difference(eval.metrics.2var$ratio))
```

The results suggest that the largest drop is between k=2 and k=3. So, let's examine the solution with k=3.

```{r}
# set the seed value
set.seed(5320)

# run the clustering for 3 clusters, iter.max=20, nstart=1000
simple.3k <- kmeans(x = retail.data1.norm, centers=3, iter.max=20, nstart=1000)

# store the (factorized) cluster value in a new variable 
retail.data1.norm$cluster <- factor(simple.3k$cluster)
```

Plot the 3-cluster solution.

```{r fig.height=4}
# plot the clusters along with their centroids
ggplot(data=retail.data1.norm, aes(x=Frozen, y=Milk, colour=cluster)) + 
  geom_point() + 
  labs(x = "Annual spending on frozen products",
       y = "Annual spending on dairy products",
       title = "Retail customers annual spending") + 
  theme_bw() +
  geom_point(data=as.data.frame(simple.3k$centers), colour="black", size=4, shape=17) 
```

Next, we examine clusters closer by looking into the cluster centers (mean) and standard deviation from the centers. When examining clusters - in order to interpret them - we typically use 'regular' (not normalized) features. The *summary.stats()* f. used below is a custom function defined in the *Utility.R*.

```{r}
# calculate the mean and sd values for all three clusters 
sum.stats <- summary.stats(feature.set = retail.data1, 
                           clusters = simple.3k$cluster, 
                           cl.num = 3)

sum.stats
```

Considering the high level of dispersion around the cluster centers (observe the high value of SD in all clusters and for both variables), it might be that the solution with K=4 clusters (the inital one) is better after all. To check if that is the case, for K=4, compute the cluster centers (mean) and dispersion (SD).

# Clustering with All Numeric Features

Since the 6 numeric attributes differ in their value ranges, we need to normalize them, that is, transform them to the value range [0,1].

```{r}
# normalize all numeric variables from the retail.data dataset
retail.norm <- as.data.frame(apply(retail.data[,c(2:7)], 2, normalize.feature))

# print the summary
summary(retail.norm)
```

Instead of guessing K, we'll right away use the Elbow method to find the optimal value for K.

```{r}
# create an empty data frame to store evaluation measures
eval.metrics.6var <- data.frame()

# run kmeans for K values in the range 2:8
for (k in 2:8) {
  set.seed(5320)
  km.res <- kmeans(x=retail.norm, centers=k, iter.max=20, nstart = 1000)
  # combine the K value and the evaluation measures, write to df
  eval.metrics.6var <- rbind(eval.metrics.6var, 
                             c(k, km.res$tot.withinss, km.res$betweenss/km.res$totss)) 
}

# assign meaningful column names
names(eval.metrics.6var) <- c("k", "tot.within.ss", "ratio")
eval.metrics.6var
```

Draw the Elbow plot.

```{r fig.height=4}
# plot the line chart for K values vs. tot.within.ss 
ggplot(data=eval.metrics.6var, aes(x=k, y=tot.within.ss)) + 
  geom_line() +
  labs(x = "\nK (cluster number)", 
       y = "Total Within Cluster Sum of Squares\n",
       title = "Reduction in error for different values of K\n") + 
  theme_bw() +
  scale_x_continuous(breaks=seq(from=0, to=8, by=1))
```

This time it is clearer that the solution with 3 clusters might be the best.
Still, we'll also examine the difference between each two consecutive values of tot.within.ss and ratio (of *between.SS* and *total.SS*).

```{r}
# calculate the difference in tot.within.ss and in ratio for each two consecutive K values
data.frame(k=c(2:8),
           tot.within.ss.delta=compute.difference(eval.metrics.6var$tot.within.ss),
           ration.delta=compute.difference(eval.metrics.6var$ratio))
```

This again suggests 3 clusters as the best option.

Let's examine in detail clustering with K=3.

```{r}
# set the seed
set.seed(5320)

# run the clustering for 3 clusters, iter.max=20, nstart=1000
retail.3k.6var <- kmeans(x=retail.norm, centers=3, iter.max=20, nstart = 1000)

# examine cluster centers
sum.stats <- summary.stats(feature.set = retail.norm, # retail.data[,c(2:7)],
                           clusters = retail.3k.6var$cluster, 
                           cl.num = 3)
sum.stats
```

```{r}
# plot the distribution of the attributes across the 3 clusters
retail.norm$Clust <- as.factor(retail.3k.6var$cluster)
create_comparison_plots(retail.norm)
```


**TASK**: try to describe the 3 groups (clusters) of customers based on the items they purchased more / less (compared to the other groups).


# Appendix: Utility Functions ('Utility.R' file)

```{r}
# The function computes the difference between each two consecutive 
# values in the given vector of values
compute.difference <- function(values) {
  dif <- list()
  dif[[1]] <- NA
  for(i in 1:(length(values)-1)) {
    dif[[i+1]] <- abs(values[i+1] - values[i])
  }
  unlist(dif)
}

# The function computes summary statistics for individual clusters
summary.stats <- function(feature.set, clusters, cl.num) {
  sum.stats <- aggregate(x = feature.set, 
                         by = list(clusters), 
                         FUN = function(x) { 
                           m <- mean(x, na.rm = T)
                           sd <- sqrt(var(x, na.rm = T))
                           paste(round(m, digits = 2), " (", 
                                 round(sd, digits = 2), ")", sep = "")
                         })
  sum.stat.df <- data.frame(cluster = sum.stats[,1], 
                            freq = as.vector(table(clusters)),
                            sum.stats[,-1])
  
  sum.stats.transpose <- t( as.matrix(sum.stat.df) )
  sum.stats.transpose <- as.data.frame(sum.stats.transpose)
  attributes <- rownames(sum.stats.transpose)
  sum.stats.transpose <- as.data.frame( cbind(attributes, sum.stats.transpose) )
  colnames(sum.stats.transpose) <- c( "attributes", rep("Mean (SD)", cl.num) )
  rownames(sum.stats.transpose) <- NULL
  sum.stats.transpose
}


# The following two functions are for creating and arranging a set of
# boxplots, one plot for each attribute that was used for clustering.
# The purpose is to visually compare the distribution of these attributes
# across the clusters
create_attr_boxplot <- function(df, attribute) {
  ggplot(data = df,
         mapping = aes(x=Clust, y=df[[attribute]], fill=Clust)) +
    geom_boxplot() + 
    labs(y = attribute) +
    theme_classic()
}

create_comparison_plots <- function(df) {
  require(dplyr)
  require(ggpubr)
  
  boxplots <- lapply(colnames(df %>% select(-Clust)), 
                     function(x) create_attr_boxplot(df, x))
  
  ggarrange(plotlist = boxplots,
            ncol = 3, nrow = 2,
            common.legend = TRUE, legend = "bottom",
            vjust = 1, hjust = -1, font.label = list(size=12))
  
}
```